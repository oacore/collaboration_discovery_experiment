{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "from urllib import urlencode\n",
    "from urllib import quote_plus\n",
    "# import urllib.parse\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "class CoreApiRequestor:\n",
    "\n",
    "    def __init__(self, endpoint, api_key):\n",
    "        self.endpoint = endpoint\n",
    "        self.api_key = api_key\n",
    "        #defaults\n",
    "        self.pagesize = 100\n",
    "        self.page = 1\n",
    "\n",
    "    def parse_response(self, decoded):\n",
    "        res = []\n",
    "        for item in decoded['data']:\n",
    "            doi = None\n",
    "            if 'identifiers' in item:\n",
    "                for identifier in item['identifiers']:\n",
    "                    if identifier and identifier.startswith('doi:'):\n",
    "                        doi = identifier\n",
    "                        break\n",
    "            res.append([item['title'], doi])\n",
    "        return res\n",
    "\n",
    "    def request_url(self, url):\n",
    "        print(url)\n",
    "        response = urlopen(url)\n",
    "        html = response.read()\n",
    "        return html\n",
    "\n",
    "    def get_method_query_request_url(self,method,query,fullText,page):\n",
    "        if (fullText):\n",
    "            fullText = 'true'\n",
    "        else:\n",
    "            fullText = 'false'\n",
    "        params = {\n",
    "            'apiKey':self.api_key,\n",
    "            'page':page,\n",
    "            'pageSize':self.pagesize,\n",
    "            'fulltext':fullText\n",
    "        }\n",
    "        return self.endpoint + method + '/' + quote_plus(query) + '?' + urlencode(params)\n",
    "\n",
    "    def get_up_to_20_pages_of_query(self,method,query,fulltext):\n",
    "        url = self.get_method_query_request_url(method,query,fulltext,1)\n",
    "        all_articles=[]\n",
    "        resp = self.request_url(url)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        all_articles.append(result)\n",
    "        if (result['totalHits']>100):\n",
    "            numOfPages = int(result['totalHits']/self.pagesize)  #rounds down\n",
    "            if (numOfPages>20):\n",
    "                numOfPages=20\n",
    "            for i in range(2,numOfPages):\n",
    "                url = self.get_method_query_request_url(method,query,False,i)\n",
    "                print(url)\n",
    "                resp =self.request_url(url)\n",
    "                all_articles.append(json.loads(resp.decode('utf-8')))\n",
    "        return all_articles\n",
    "    \n",
    "    def get_repository_articles_fulltext_as_dict(self,repository_id):\n",
    "        return \"\"\n",
    "    \n",
    "    def get_search_repository_request_url(self,repoQuery,page=1,pageSize=10):\n",
    "        params={\n",
    "            'apiKey':self.api_key,\n",
    "            'page':page,\n",
    "            'pageSize':pageSize\n",
    "        }\n",
    "        return self.endpoint + \"/repositories/search/\"+quote_plus(repoQuery)+'?'+urlencode(params)\n",
    "    \n",
    "    def search_repository_ids_by_name(self,repoName):\n",
    "        discoverRepoUrl = self.get_search_repository_request_url(repoName)\n",
    "        resp = self.request_url(discoverRepoUrl)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        for item in result['data']:\n",
    "            if 'name' in item:\n",
    "                name = item['name']\n",
    "            if 'id' in item:\n",
    "                id = item['id']\n",
    "            repos[id]=name\n",
    "        return repos\n",
    "    \n",
    "    def get_count_articles_of_repository_url(self,repoId,withFullText):\n",
    "        ft = 'false'\n",
    "        if(withFullText):\n",
    "            ft = 'true'\n",
    "                \n",
    "        params = {\n",
    "            'apiKey':self.api_key,\n",
    "            'fulltext':ft\n",
    "        }    \n",
    "        return self.endpoint + \"/articles/search/repositories.id:\"+repoId+'?'+urlencode(params)\n",
    "    \n",
    "    def count_articles_of_repository(self,repoId,withFullText=False):\n",
    "        countArticlesOfRepoUrl = self.get_count_articles_of_repository_url(repoId,withFullText)\n",
    "        print(countArticlesOfRepoUrl)\n",
    "        resp = self.request_url(countArticlesOfRepoUrl)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        return result['totalHits']\n",
    "    \n",
    "    def get_url_of_download_articles_of_repository(self,repoId,fullText,page,pageSize):\n",
    "        params={\n",
    "            'apiKey':self.api_key,\n",
    "            'page':page,\n",
    "            'pageSize':pageSize,\n",
    "            'fulltext':fullText\n",
    "               }\n",
    "        return self.endpoint + \"/articles/search/repositories.id:\"+str(repoId)+'?'+urlencode(params)\n",
    "    \n",
    "    def download_articles_of_repository(self, repoId, fulltext=True, page=1, pageSize=100):\n",
    "        url = self.get_url_of_download_articles_of_repository(repoId,fulltext,page,pageSize)\n",
    "        all_articles=[]\n",
    "        resp = self.request_url(url)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        if (result['totalHits']>100):\n",
    "            numOfPages = int(result['totalHits']/self.pagesize)  #rounds down\n",
    "            if (numOfPages>5):\n",
    "                numOfPages=5\n",
    "            for i in range(2,numOfPages):\n",
    "                url = self.get_url_of_download_articles_of_repository(repoId,fulltext,page,pageSize)\n",
    "                print(url)\n",
    "                resp =self.request_url(url)\n",
    "                all_articles.append(json.loads(resp.decode('utf-8')))\n",
    "        return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initialise parameters\n",
    "'''\n",
    "# init \n",
    "endpoint = 'https://core.ac.uk/api-v2'\n",
    "\n",
    "'''\n",
    "********************************************\n",
    "Add your own api key below\n",
    "'''\n",
    "api_key =\"\"\n",
    "# or get it from a config file\n",
    "file = open(\"api_key.secret\",\"r\") \n",
    "api_key=file.read()\n",
    "api_key=\"20hIsS1F5j4D2C2iXrg4Wxf7VTp4Xt1j\"\n",
    "api_key.strip()\n",
    "'''\n",
    "********************************************\n",
    "'''\n",
    "'''\n",
    "Create your api object\n",
    "'''\n",
    "api = CoreApiRequestor(endpoint,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def cache_repository_articles(repoId,numPages=10):\n",
    "    '''\n",
    "    Use page size 50 \n",
    "    -\n",
    "    store as pickle every 200 articles (4 pages)\n",
    "    in the end you will have in the /data foledr files:\n",
    "    all_articles_on1_1.pkl\n",
    "    all_articles_on1_2.pkl\n",
    "    all_articles_on1_3.pkl\n",
    "    ...\n",
    "\n",
    "    '''\n",
    "    all_articles=[]\n",
    "    for i in range(1,numPages*4):\n",
    "        url = api.get_url_of_download_articles_of_repository(repoId,True,i,50)\n",
    "        response = urlopen(url)\n",
    "        resp = response.read()\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        all_articles.append(result)\n",
    "        if (i%4==0):\n",
    "            pickle.dump(all_articles,open('data/all_articles_on'+str(repoId)+'_'+str(i/4)+'.pkl','wb'),pickle.HIGHEST_PROTOCOL)\n",
    "            all_articles=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-> aberdeen\n",
    "# 86-> Open university\n",
    "# 39 -> edinburgh \n",
    "# 136 -> warwick\n",
    "# 48 -> glascow\n",
    "repos_to_cache=[1,86,39,136]\n",
    "# repos_to_cache=[48]\n",
    "for repo in repos_to_cache:\n",
    "    cache_repository_articles(repo,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://core.ac.uk/api-v2/articles/search/repositories.id:1?fulltext=True&apiKey=20hIsS1F5j4D2C2iXrg4Wxf7VTp4Xt1j&page=1&pageSize=10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.get_url_of_download_articles_of_repository(1,True,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
